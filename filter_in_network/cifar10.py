# -*- coding: utf-8 -*-
"""CIFAR10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RiZR_agz0vcB_6OcPmex9_y1mVHJHdXe

## Training model for CIFAR10
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from keras.models import Sequential, load_model
from keras.datasets import cifar10
from keras.utils import np_utils
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.callbacks import ModelCheckpoint

(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()
x_train = X_train.astype('float32')/255
x_test = X_test.astype('float32')/255
y_train = np_utils.to_categorical(Y_train)
y_test = np_utils.to_categorical(Y_test)

model = Sequential()
model.add(Conv2D(filters=64, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))
model.add(Conv2D(filters=64, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=2))

model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))
model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=2))

model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))
model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))
model.add(MaxPool2D(pool_size=2))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(rate=0.25))
model.add(Dense(10, activation='softmax'))

print(model.summary())

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_checkpoint = ModelCheckpoint('drive/MyDrive/class_Image/HW4/model.hdf5', monitor='val_loss', verbose=1, save_best_only=False)
model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(x_test, y_test)
print('Test:')
print('Loss:', loss)
print('Accuracy:', accuracy)

"""## Get three kernels from model after training"""

from keras.models import load_model

model = load_model('drive/MyDrive/class_Image/HW4/model.hdf5')

"""#### layer 0"""

layer_num = 0

all_kernels, biases = model.layers[layer_num].get_weights()
print(all_kernels.shape)
print(biases.shape)

import random

chosen_kernels = []

for i in range(20):
  chosen_kernels.append( all_kernels[:,:,:,i] )

for i in range(len(chosen_kernels)):
  print(i)
  print(chosen_kernels[i])
  print()

import cv2
import numpy as np
from pathlib import Path

COLOR = 3
KERNEL_SIZE = 3

# read image
origin_img = cv2.imread( 'drive/MyDrive/class_Image/HW4/images/cupcake.jpg' )

# convolution
height = origin_img.shape[0]
width = origin_img.shape[1]

cov_imgs = np.zeros( [ len(chosen_kernels), height+4, width+4, 3 ]) # add padding

cov_range = int((KERNEL_SIZE-1)/2)
for i in range(len(chosen_kernels)):
  kernel = chosen_kernels[i]
  for y in range(cov_range, height-cov_range):
      for x in range(cov_range, width-cov_range):
          part_img = origin_img[ y-1:y+2, x-1:x+2, : ]
          for color in range(0, COLOR):
              value = abs(np.sum(part_img[:,:,color] * kernel[:,:,color]))
              cov_imgs[i,y,x,color] = value
  print('covolution : kernel', i, 'done.')

cov_imgs = cov_imgs[:, KERNEL_SIZE-1:-(KERNEL_SIZE-1), KERNEL_SIZE-1:-(KERNEL_SIZE-1),:] # remove padding

# write convolution images
output_dir = Path.cwd() / 'drive/MyDrive/class_Image/HW4/images/output_img'
Path.mkdir( output_dir, exist_ok=True)

for i in range(cov_imgs.shape[0]):
  cv2.imwrite( str(output_dir)+'/kernel_'+str(layer_num)+'_'+str(i)+'.jpg', cov_imgs[i] )

print('save images done.')

"""#### layer 0 -grey"""

layer_num = 0

all_kernels, biases = model.layers[layer_num].get_weights()
print(all_kernels.shape)
print(biases.shape)

import random

chosen_kernels = []

for i in range(10,20):
  chosen_kernels.append( all_kernels[:,:,:,i] )

for i in range(len(chosen_kernels)):
  print(i)
  print(chosen_kernels[i])
  print()

import cv2
import numpy as np
from pathlib import Path

COLOR = 3
KERNEL_SIZE = 3

# read image
origin_img = cv2.imread( 'drive/MyDrive/class_Image/HW4/images/cupcake.jpg' )

# convolution
height = origin_img.shape[0]
width = origin_img.shape[1]

cov_imgs = np.zeros( [ len(chosen_kernels), height+4, width+4, 3 ]) # add padding

cov_range = int((KERNEL_SIZE-1)/2)
for i in range(len(chosen_kernels)):
  kernel = chosen_kernels[i]
  for y in range(cov_range, height-cov_range):
      for x in range(cov_range, width-cov_range):
          part_img = origin_img[ y-1:y+2, x-1:x+2, : ]
          for color in range(0, COLOR):
              value = abs(np.sum(part_img[:,:,color] * kernel[:,:,color]))
              cov_imgs[i,y,x,color] = value
  print('covolution : kernel', i, 'done.')

cov_imgs = cov_imgs[:, KERNEL_SIZE-1:-(KERNEL_SIZE-1), KERNEL_SIZE-1:-(KERNEL_SIZE-1),:] # remove padding

# write convolution images
output_dir = Path.cwd() / 'drive/MyDrive/class_Image/HW4/images/output_img'
Path.mkdir( output_dir, exist_ok=True)

for i in range(cov_imgs.shape[0]):
  cv2.imwrite( str(output_dir)+'/kernel_'+str(layer_num)+'_grey0_'+str(i)+'.jpg', cov_imgs[i,:,:,0] )
  cv2.imwrite( str(output_dir)+'/kernel_'+str(layer_num)+'_grey1_'+str(i)+'.jpg', cov_imgs[i,:,:,1] )
  cv2.imwrite( str(output_dir)+'/kernel_'+str(layer_num)+'_grey2_'+str(i)+'.jpg', cov_imgs[i,:,:,2] )

print('save images done.')